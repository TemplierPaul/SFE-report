% This file was created with JabRef 2.8.1.
% Encoding: UTF8
@misc{wilson2018positionalcgp,
    title={Positional Cartesian Genetic Programming},
    author={DG Wilson and Julian F. Miller and Sylvain Cussat-Blanc and Hervé Luga},
    year={2018},
    eprint={1810.04119},
    archivePrefix={arXiv},
    primaryClass={cs.NE}
}


@report{NEAT_2,
   abstract = {An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolu-tion of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incremen-tally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.},
   author = {Kenneth O Stanley and Risto Miikkulainen},
   keywords = {Genetic algorithms,competing conventions,network topologies,neural networks,neuroevolution,speciation},
   title = {Evolving Neural Networks through Augmenting Topologies},
   url = {http://mitpress.mit.edu/e-mail},
}
@article{CMAES-Atari,
   abstract = {We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.},
   author = {Tim Salimans and Jonathan Ho and Xi Chen and Szymon Sidor and Ilya Sutskever},
   month = {3},
   title = {Evolution Strategies as a Scalable Alternative to Reinforcement Learning},
   url = {http://arxiv.org/abs/1703.03864},
   year = {2017},
}
@report{NEAT_1,
   abstract = {Neuroevolution, i.e. evolving artificial neural networks with genetic algorithms, has been highly effective in reinforcement learning tasks, particularly those with hidden state information. An important question in neuraevolu-tion is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT) that outperforms the best fixed-topology methods on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incremen-tally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAS because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, making it possible to evolve increasingly complex solutions over time, thereby strengthening the analogy with biological evolution.},
   author = {Kenneth 0 Stanley and Risto Miikkulainen},
   title = {Efficient Evolution of Neural Network Topologies},
}
@report{rtNEAT,
   abstract = {In most modern video games, character behavior is scripted; no matter how many times the player exploits a weakness, that weakness is never repaired. Yet if game characters could learn through interacting with the player, behavior could improve as the game is played, keeping it interesting. This paper introduces the real-time NeuroEvolution of Augmenting Topologies (rtNEAT) method for evolving increasingly complex artificial neural networks in real time, as a game is being played. The rtNEAT method allows agents to change and improve during the game. In fact, rtNEAT makes possible an entirely new genre of video games in which the player trains a team of agents through a series of customized exercises. To demonstrate this concept, the NeuroEvolving Robotic Operatives (NERO) game was built based on rtNEAT. In NERO, the player trains a team of virtual robots for combat against other players' teams. This paper describes results from this novel application of machine learning, and demonstrates that rtNEAT makes possible video games like NERO where agents evolve and adapt in real time. In the future, rtNEAT may allow new kinds of educational and training applications through interactive and adapting games.},
   author = {Kenneth O Stanley and Bobby D Bryant and Risto Miikkulainen},
   issue = {6},
   journal = {IEEE Transactions on Evolutionary Computation (Special Issue on Evolutionary Computation and Games)},
   title = {Real-Time Neuroevolution in the NERO Video Game},
   volume = {9},
   url = {http://nerogame.org},
   year = {2005},
}

@report{FS-NEAT,
   abstract = {Feature selection is the process of finding the set of inputs to a machine learning algorithm that will yield the best performance. Developing a way to solve this problem automatically would make current machine learning methods much more useful. Previous efforts to automate feature selection rely on expensive meta-learning or are applicable only when labeled training data is available. This paper presents a novel method called FS-NEAT which extends the NEAT neuroevolution method to automatically determine an appropriate set of inputs for the networks it evolves. By learning the network's inputs, topology, and weights simultaneously , FS-NEAT addresses the feature selection problem without relying on meta-learning or labeled data. Initial experiments in an autonomous car racing simulation demonstrate that FS-NEAT can learn better and faster than regular NEAT. In addition, the networks it evolves are smaller and require fewer inputs. Furthermore, FS-NEAT's performance remains robust even as the feature selection task it faces is made increasingly difficult.},
   author = {Shimon Whiteson and Peter Stone and Kenneth O Stanley and Risto Miikkulainen and Nate Kohl},
   keywords = {I26 [Artificial Intelligence]: Learning-Connectionism and neural nets General Terms Experimentation Keywords genetic algorithms,feature selection,neural networks},
   title = {Automatic Feature Selection in Neuroevolution},
   year = {2005},
}

@article{GP_anecdotes,
   abstract = {Biological evolution provides a creative fount of complex and subtle adaptations, often surprising the scientists who discover them. However, because evolution is an algorithmic process that transcends the substrate in which it occurs, evolution's creativity is not limited to nature. Indeed, many researchers in the field of digital evolution have observed their evolving algorithms and organisms subverting their intentions, exposing unrecognized bugs in their code, producing unexpected adaptations, or exhibiting outcomes uncannily convergent with ones in nature. Such stories routinely reveal creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This paper is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems.},
   author = {Joel Lehman and Jeff Clune and Dusan Misevic and Christoph Adami and Lee Altenberg and Julie Beaulieu and Peter J. Bentley and Samuel Bernard and Guillaume Beslon and David M. Bryson and Patryk Chrabaszcz and Nick Cheney and Antoine Cully and Stephane Doncieux and Fred C. Dyer and Kai Olav Ellefsen and Robert Feldt and Stephan Fischer and Stephanie Forrest and Antoine Frénoy and Christian Gagné and Leni Le Goff and Laura M. Grabowski and Babak Hodjat and Frank Hutter and Laurent Keller and Carole Knibbe and Peter Krcah and Richard E. Lenski and Hod Lipson and Robert MacCurdy and Carlos Maestre and Risto Miikkulainen and Sara Mitri and David E. Moriarty and Jean-Baptiste Mouret and Anh Nguyen and Charles Ofria and Marc Parizeau and David Parsons and Robert T. Pennock and William F. Punch and Thomas S. Ray and Marc Schoenauer and Eric Shulte and Karl Sims and Kenneth O. Stanley and François Taddei and Danesh Tarapore and Simon Thibault and Westley Weimer and Richard Watson and Jason Yosinski},
   month = {3},
   title = {The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities},
   url = {http://arxiv.org/abs/1803.03453},
   year = {2018},
}
@article{Dota2,
   abstract = {The capacity of genetic programming (GP) to evolve a 'hero' character in the Dota 2 video game is investigated. A reinforcement learning context is assumed in which the only input is a 320-dimensional state vector and performance is expressed in terms of kills and net worth. Minimal assumptions are made to initialize the GP game playing agents-evolution from a tabula rasa starting point-implying that: 1) the instruction set is not task specific; 2) end of game performance feedback reflects quantitive properties a player experiences ; 3) no attempt is made to impart game specific knowledge into GP, such as heuristics for improving navigation, minimizing partial observability, improving team work or prioritizing the protection of specific strategically important structures. In short, GP has to actively develop its own strategies for all aspects of the game. We are able to demonstrate competitive play with the built in game opponents assuming 1-on-1 competitions using the 'Shadow Fiend' hero. The single most important contributing factor to this result is the provision of external memory to GP. Without this, the resulting Dota 2 bots are not able to identify strategies that match those of the built-in game bot. CCS CONCEPTS • Computing methodologies → Reinforcement learning; Genetic programming.},
   author = {Robert J Smith and Malcolm I Heywood},
   doi = {10.1145/3321707.3321866},
   isbn = {9781450361118},
   keywords = {Coevolution,Dota 2,External Memory,Genetic programming,Partial Observability,Reinforcement learning},
   publisher = {ACM},
   title = {Evolving Dota 2 Shadow Fiend Bots using Genetic Programming with External Memory},
   url = {https://doi.org/10.1145/3321707.3321866},
   year = {2019},
}
@report{GRN,
   abstract = {Over the past twenty years, artificial Gene Regulatory Networks (GRNs) have shown their capacity to solve real-world problems in various domains such as agent control, signal processing and artificial life experiments. They have also benefited from new evolutionary approaches and improvements to dynamic which have increased their optimization efficiency. In this paper, we present an additional step toward their usability in machine learning applications. We detail an GPU-based implementation of differentiable GRNs, allowing for local optimization of GRN architectures with stochastic gradient descent (SGD). Using a standard machine learning dataset, we evaluate the ways in which evolution and SGD can be combined to further GRN optimization. We compare these approaches with neural network models trained by SGD and with support vector machines.},
   author = {D G Wilson and Kyle Harrington and Sylvain Cussat-Blanc and Hervé Luga},
   title = {Evolving Differentiable Gene Regulatory Networks},
   url = {https://github.com/d9w/pyGRN},
}
@report{HyperNEAT,
   abstract = {Research in neuroevolution, i.e. evolving artificial neural networks (ANNs) through evolutionary algorithms , is inspired by the evolution of biological brains. Because natural evolution discovered intelligent brains with billions of neurons and trillions of connections, perhaps neuroevolution can do the same. Yet while neuroevolution has produced successful results in a variety of domains, the scale of natural brains remains far beyond reach. This paper presents a method called Hypercube-based NeuroEvolution of Augmenting Topologies (HyperNEAT) that aims to narrow this gap. HyperNEAT employs an indirect encoding called connective Compositional Pattern Producing Networks (connective CPPNs) that can produce con-nectivity patterns with symmetries and repeating motifs by interpreting spatial patterns generated within a hypercube as connectivity patterns in a lower-dimensional space. The advantage of this approach is that it can exploit the geometry of the task by mapping its regularities onto the topology of the network, thereby shifting problem difficulty away from dimensionality to underlying problem structure. Furthermore, connective CPPNs can represent the same connectivity pattern at any resolution, allowing ANNs to scale to new numbers of inputs and outputs without further evolution. HyperNEAT is demonstrated through visual discrimination and food gathering tasks, including successful visual discrimination networks containing over eight million connections. The main conclusion is that the ability to explore the space of regular connectivity patterns opens up a new class of complex high-dimensional tasks to neuroevolution.},
   author = {Kenneth O Stanley and David D ' Ambrosio and Jason Gauci},
   issue = {2},
   journal = {Artificial Life journal},
   keywords = {CPPNs,Compositional Pattern Producing Networks,HyperNEAT,generative and developmental systems,indirect encoding,large-scale artificial neural networks},
   publisher = {MIT Press},
   title = {A Hypercube-Based Indirect Encoding for Evolving Large-Scale Neural Networks},
   volume = {15},
   year = {2009},
}

@report{CMAES_DL,
   abstract = {Hyperparameters of deep neural networks are often optimized by grid search, random search or Bayesian optimization. As an alternative, we propose to use the Co-variance Matrix Adaptation Evolution Strategy (CMA-ES), which is known for its state-of-the-art performance in derivative-free optimization. CMA-ES has some useful invariance properties and is friendly to parallel evaluations of solutions. We provide a toy example comparing CMA-ES and state-of-the-art Bayesian optimization algorithms for tuning the hyperparameters of a convolutional neural network for the MNIST dataset on 30 GPUs in parallel. Hyperparameters of deep neural networks (DNNs) are often optimized by grid search, random search (Bergstra & Bengio, 2012) or Bayesian optimization (Snoek et al., 2012a; 2015). For the optimization of continuous hyperparameters, Bayesian optimization based on Gaussian processes (Ras-mussen & Williams, 2006) is known as the most effective method. While for joint structure search and hyperparameter optimization, tree-based Bayesian optimization methods (Hutter et al., 2011; Bergstra et al., 2011) are known to perform better (Bergstra et al.; Eggensperger et al., 2013; Domhan et al., 2015), here we focus on continuous optimization. We note that integer parameters with rather wide ranges (e.g., number of filters) can, in practice, be considered to behave like continuous hyper-parameters. As the evaluation of a DNN hyperparameter setting requires fitting a model and evaluating its performance on validation data, this process can be very expensive, which often renders sequential hyperparameter optimization on a single computing unit infeasible. Unfortunately, Bayesian optimization is sequential by nature: while a certain level of parallelization is easy to achieve by conditioning decisions on expectations over multiple hallucinated performance values for currently running hyperparameter evaluations (Snoek et al., 2012a) or by evaluating the optima of multiple acquisition functions concurrently (Hutter et al., 2012; Chevalier & Ginsbourger, 2013; Desautels et al., 2014), perfect parallelization appears difficult to achieve since the decisions in each step depend on all data points gathered so far. Here, we study the use of a different type of derivative-free continuous optimization method where parallelism is allowed by design. The Covariance Matrix Adaptation Evolution Strategy (CMA-ES (Hansen & Ostermeier, 2001)) is a state-of-the-art optimizer for continuous black-box functions. While Bayesian optimization methods often perform best for small function evaluation budgets (e.g., below 10 times the number of hyperparameters being optimized), CMA-ES tends to perform best for larger function evaluation budgets; for example, Loshchilov et al. (2013) showed that CMA-ES performed best among more than 100 classic and modern optimizers on a wide range of blackbox functions. CMA-ES has also been used for hyperparameter tuning before, e.g., for tuning its own Ranking SVM surrogate models (Loshchilov et al.},
   author = {Ilya Loshchilov and Frank Hutter},
   title = {CMA-ES FOR HYPERPARAMETER OPTIMIZATION OF DEEP NEURAL NETWORKS},
   url = {https://sites.google.com/site/cmaesfordnn/},
}

@article{CPPN,
   author = {Kenneth O. Stanley},
   title = {Compositional pattern producing networks: A novel abstraction of development},
}
@report{competitive_co-evolution,
   abstract = {Two major goals in machine learning are the discovery and improvement of solutions to complex problems. In this paper, we argue that complexification, i.e. the incremental elaboration of solutions through adding new structure, achieves both these goals. We demonstrate the power of complexification through the NeuroEvolution of Augmenting Topologies (NEAT) method, which evolves increasingly complex neural network architectures. NEAT is applied to an open-ended coevolutionary robot duel domain where robot controllers compete head to head. Because the robot duel domain supports a wide range of strategies, and because coevolution benefits from an escalating arms race, it serves as a suitable testbed for studying complexification. When compared to the evolution of networks with fixed structure , complexifying evolution discovers significantly more sophisticated strategies. The results suggest that in order to discover and improve complex solutions, evolution, and search in general, should be allowed to complexify as well as optimize.},
   author = {Kenneth O Stanley and Risto Miikkulainen},
   journal = {Journal of Artificial Intelligence Research},
   keywords = {Changes and compilation copyright © The American Association for Artificial Intelligence. All rights reserved.,Copyright © 2004 AI Access},
   pages = {63-100},
   title = {Competitive Coevolution through Evolutionary Complexification},
   volume = {21},
   year = {2004},
}
@article{Games_AI,
   abstract = {s Although one of the fundamental goals of AI is to understand and develop intelligent systems that have all the capabilities of humans, there is little active research directly pursuing this goal. We propose that AI for interactive computer games is an emerging application area in which this goal of human-level AI can successfully be pursued. Interactive computer games have increasingly complex and realistic worlds and increasingly complex and intelligent computer-controlled characters. In this article, we further motivate our proposal of using interactive computer games for AI research, review previous research on AI and games, and present the different game genres and the roles that human-level AI could play within these genres. We then describe the research issues and AI techniques that are relevant to each of these roles. Our conclusion is that interactive computer games provide a rich environment for incremental research on human-level AI. O ver the last 30 years, research in AI has fragmented into more and more specialized fields, working on more and more specialized problems, using more and more specialized algorithms. This approach has led to a long string of successes with important theoretical and practical advancements. However, these successes have made it easy for us to ignore our failure to make significant progress in building human-level AI systems. Human-level AI systems are the ones that you dreamed about when you first heard of AI: HAL from 2001, A Space Odyssey; DATA from Star Trek; or CP30 and R2D2 from Star Wars. They are smart enough to be both triumphant heroes and devious villains. They seamlessly integrate all the human-level capabilities: real-time response, robustness, autonomous intelligent interaction with their environment, planning, communication with natural language, com-monsense reasoning, creativity, and learning. If this is our dream, why isn't any progress being made? Ironically, one of the major reasons that almost nobody (see Brooks et al. [2000] for one high-profile exception) is working on this grand goal of AI is that current applications of AI do not need full-blown human-level AI. For almost all applications, the generality and adaptability of human thought is not needed-specialized, although more rigid and fragile, solutions are cheaper and easier to develop. Unfortunately, it is unclear whether the approaches that have been developed to solve specific problems are the right building blocks for creating human-level intelligence. The thesis of this article is that interactive computer games are the killer application for human-level AI. They are the application that will need human-level AI. Moreover, they can provide the environments for research on the right kinds of problem that lead to the type of incremental and integrative research needed to achieve human-level AI. Computer-Generated Forces Given that our personal goal is to build human-level AI systems, we have struggled to find the right application for our research that requires the breadth, depth, and flexibility of human-level intelligence. In 1991, we found computer-generated forces for large-scale distributed simulations as a potential application. Effective military training requires a complete battle space with tens if not hundreds or thousands of participants. The real world is too expensive and dangerous to use for continual training,},
   author = {John E Laird and Michael Van Lent},
   journal = {AI Magazine Volume 22 Number 2 (2001) (© AAAI)},
   title = {Human-Level AI's Killer Application Interactive Computer Games },
   year = {2001},
}

@article{Atari,
  author    = {Marc G. Bellemare and
               Yavar Naddaf and
               Joel Veness and
               Michael Bowling},
  title     = {The Arcade Learning Environment: An Evaluation Platform for General
               Agents},
  journal   = {CoRR},
  volume    = {abs/1207.4708},
  year      = {2012},
  url       = {http://arxiv.org/abs/1207.4708},
  archivePrefix = {arXiv},
  eprint    = {1207.4708},
  timestamp = {Mon, 13 Aug 2018 16:46:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1207-4708.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{agent57,
    title={Agent57: Outperforming the Atari Human Benchmark},
    author={Adrià Puigdomènech Badia and Bilal Piot and Steven Kapturowski and Pablo Sprechmann and Alex Vitvitskyi and Daniel Guo and Charles Blundell},
    year={2020},
    eprint={2003.13350},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{DQN,
    title={Playing Atari with Deep Reinforcement Learning},
    author={Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
    year={2013},
    eprint={1312.5602},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{deep_neuroevo,
  author    = {Felipe Petroski Such and
               Vashisht Madhavan and
               Edoardo Conti and
               Joel Lehman and
               Kenneth O. Stanley and
               Jeff Clune},
  title     = {Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative
               for Training Deep Neural Networks for Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1712.06567},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.06567},
  archivePrefix = {arXiv},
  eprint    = {1712.06567},
  timestamp = {Mon, 13 Aug 2018 16:46:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1712-06567.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{BERL,
    author       = {Dennis Wilson},
    title        = {{BERL.lj: Benchmarking Evolutionary Reinforcement Learning}},
    url          = {https://github.com/d9w/BERL.jl}
    }
    
@inproceedings{CMA-ES,
  title={Adapting arbitrary normal mutation distributions in evolution strategies: The covariance matrix adaptation},
  author={Hansen, Nikolaus and Ostermeier, Andreas},
  booktitle={Proceedings of IEEE international conference on evolutionary computation},
  pages={312--317},
  year={1996},
  organization={IEEE}
}