\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
\markboth{Conclusion}{Conclusion}
\label{chap:conclusion}
%\minitoc

While comparing evolutionary RL algorithms is a critical part of the understanding of the state of the art and the development of new algorithms in this field, a standardized benchmarking framework has not emerged yet, which is the goal of the development of \berl. 

More than in the development of the architecture, the challenge also relies in the choices of the tasks to use for the comparison. Standard environments such as Gym's CartPole provide a well-known benchmark, but have been solved by multiple methods. On the other hand, games like Dota 2 are arduous to tackle both for the algorithms, due to the very high complexity of the game and its short- and long-term strategies, and for the researchers willing to use them because of the challenges raising from the interactions with the game. Even though finding a method solving Dota 2 can be interesting for research, its capacity to be applied to other problems, especially real-world tasks, still requires a standardized benchmark. 

On the personal side, this internship has given me the opportunity to learn more about the field of Evolutionary Computation, which I had been attracted toward for many years. I got to discover both the quality and the diversity of the work on this topic, and more generally on Artificial Life, through reading bibliography and attending conferences. 

Neuroevolution methods have opened to me a different approach to Reinforcement Learning and its questions, building new techniques on algorithmic ideas discovered decades ago and based on natural behaviors evolved for millions of years. The SuReLI team has also been a great inspiration source, joining multiple approaches and competences in the same goal of improving RL research, in a skilled and enjoyable working environment. 

I have thoroughly enjoyed working on this project, and I look forward to keeping working with Dennis Wilson and Emmanuel Rachelson in the next years, since I am set to continue this internship with a PhD on bio-inspired methods for Artificial Neural Network learning. With both of them as supervisors, we will tackle the issue of finding policies that help networks actually learn instead of solely optimize for a specific task, based on biological neuronal models and notably building on previous work in Neuromodulation \cite{neuromodulation}. 

% DGW: COCO (https://coco.gforge.inria.fr/)
% McDermott, James, et al. "Genetic programming needs better benchmarks." Proceedings of the 14th annual conference on Genetic and evolutionary computation. 2012.


% DGW: this is a very nice conclusion but doesn't conclude the subject of the internship. Can you make some conclusions about the state of evolutionary reinforcement learning? About the benchmarks used? About neuroevolution? Then, with those conclusions in mind, what are your next steps?

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "isae-report-template"
%%% End: 
