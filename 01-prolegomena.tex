\chapter{Prolegomena}
\label{chap:prolegomena}

\section{Reinforcement Learning: what is an MDP?}

A \textbf{Markov Decision Process} \cite{mdp} (or \textit{MDP}) is a process that formally describes a fully-observable environment, which means the current state fully characterizes the environment. 

It can be defined as:

\begin{itemize}
    \item a set of states $S$
    \item a set of actions $A$ that can be taken from the states in $S$
    \item the probability $P_a(s, s')$ of reaching each state $s'$ if action $a$ is chosen from state $s$
    \item and the immediate reward $R_a(s, s')$ associated with transitioning from state $s$ to state $s'$ by taking actions $a$
\end{itemize}
      
\begin{figure}[H]
    \centering
    \begin{tikzpicture}
  \node[draw] (A) at (0cm,0cm) {State s + Action s};
  \node[draw] (B) at (3cm,1.5cm) {State s'};
  \node[draw] (C) at (3cm,-1.5cm) {State s''};

  
  \node[draw,fill=yellow] (Arga) at (1.5cm,2.5cm) {Proba $p_1$};
  \node[draw,fill=yellow] (Argb) at (1.5cm,-2.5cm) {Proba $p_2$};
  
  \draw node[vertex] (Jointa) at (1.5cm,0.75cm) {};
  \draw node[vertex] (Jointb) at (1.5cm, -0.75cm) {};
  
  \draw[->,draw=blue] (A) to (B);
  \draw[->,draw=blue] (A) to (C);
  
  \draw[-,draw=black] (Arga) to (Jointa);
  \draw[-,draw=black] (Argb) to (Jointb);
  
  \end{tikzpicture}
  
    \caption{Markov Decision Process ($p_1 + p_2 = 1$)}
    \label{fig:my_label}
\end{figure}

A (deterministic) \textbf{policy} $\pi$ is a mapping from states to actions. $\pi: S \longrightarrow A$.

The goal of \textbf{Reinforcement Learning} is to find the optimal policy $\pi^*$ that maximizes the expected sum of rewards across an experiment. 

\section{Neural Networks acting as a policy}

As neural networks \cite{perceptron} can - in theory - approximate any function, they can be used to define a policy mapping any state to an action. The \textbf{Direct Policy search} approach aims at finding the best policy through the optimization of the parameters of this policy network.

In this method, the network is optimized to output the best action to take, from a state given as input. Two optimization methods can be employed: gradient-based, or stochastic.

\subsection{Gradient-based optimization}

\textbf{Policy gradient methods} use gradient descent \cite{sgd} on the parameters of the neural network to optimize the policy. 

As in supervised learning the true value of the target is known, the gradient is readily available, making gradient descent a viable option for neural network optimization. 

However, reinforcement learning environments present harder settings for gradient-based optimization:

\begin{itemize}
    \item The best action is not already known, so the gradient is harder to compute
    \item As a first-order method, gradient descent can get stuck in a local optimum or a plateau
    \item RL environments with sparse rewards present many plateaus and make the gradient harder to find
\end{itemize}

Hence, optimizations methods not based on gradient such as evolutionary methods can be a solution to these sorts of problems.

\subsection{Evolutionary Reinforcement Learning}
\label{sec:ERL}

\textbf{Evolutionary Reinforcement Learning} (or \textit{ERL}) is the RL approach based on using stochastic optimization, and more precisely evolutionary algorithms, to optimize a policy network without computing the gradient. Some of the evolutionary algorithms ERL can use are described in chapter \ref{chap:evo}.

As the ERL field is still lacking a tool to compare and benchmark algorithms, I took part in a Dota 2 ERL competition and I am currently working on a benchmarking framework, as described in chapters. \ref{chap:dota} and \ref{chap:berl}.

\section{Development tools}
\subsection{Julia}
    
\addlink{https://julialang.org/}{\textbf{Julia}} is a high-level, dynamic programming language designed for high performance with a just-in-time compiler. It is notably interoperable with multiple languages, including C, Fortran, Python, R, MATLAB, Java, or Scala. \cite{julia-lang}

I used Julia in this project to provide a faster environment than Python, while still being compatible with state-of-the-art Python libraries such as gym or Atari.
\\

The principles of \textbf{Test-Driven Development} were followed: requirements are first translated into tests, then the code is developed until all tests pass. Tests provide both a way to ensure new code doesn't break previously working code, and a list of use cases of the library, hence improving its documentation.

Additionally a \addlink{https://travis-ci.org/}{\textbf{TravisCI}} instance was linked to all libraries I developed to build and test all code on remote servers, ensuring they can be run on machines with a clean configuration. 

\subsection{Cambrian}

\addlink{https://github.com/d9w/Cambrian.jl}{\textbf{Cambrian.jl}} is an Evolutionary Computation framework developed in Julia by my supervisor Dennis Wilson. It implements a structure to facilitate the development of evolutionary algorithms, focusing on genetic programming and neuroevolution. 

I used Cambrian in my development to provide a consistent framework between projects and ensure compatibility, and for its usefulness in structuring the development.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "isae-report-template"
%%% End: 