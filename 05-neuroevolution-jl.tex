\chapter{Developping NeuroEvolution.jl}
\label{chap:neuroevo}

\addlink{https://github.com/TemplierPaul/NeuroEvolution.jl}{NeuroEvolution.jl} is a Julia library I developed to implement multiple neuroevolution algorithms for BERL. It includes \textbf{NEAT} (NeuroEvolution of Augmenting Topologies) and \textbf{HyperNEAT} (Hypercube-based NEAT), and an implementation of a direct weights optimization in a fix network using \textbf{CMAES} (Covariance matrix adaptation evolution strategy) is planned to complete it. 

The concepts powering the NEAT algorithm may seem relatively simple and clear as described in the literature, making it a quite elegant solution. \\
However, its implementation raises multiple complex issues, turning out to be rather intricate. The obstacles met when implementing part of it during an earlier \addlink{https://github.com/TemplierPaul/Genepy}{personal project},  and in the \addlink{https://github.com/d9w/NEAT.jl/tree/master/src}{NEAT agent} developed for the Breezy competition (cf chapter \ref{chap:dota}) helped me design a more reliable structure and reduced the development time.

This chapter aims at presenting the original NEAT algorithm, both in the theory and in its implementation, and then explaining choices made during the design and development of NeuroEvolution.jl, its architecture, and the main principles ruling it. 

\section{Original NEAT algorithm}
\label{sec:og-neat}

The NEAT algorithm relies on 3 main solutions to neuroevolution issues:
\begin{itemize}
    \item \textbf{Crossover between different structures}: NEAT provides a genotype-to-phenotype mapping based on mutations (Fig. \ref{fig:NEAT} in chapter \ref{chap:evo}).
    \item \textbf{Protection of topological innovation}: The speciation mechanism evaluates candidates by comparing them to similar architectures to allow new structures to mature without being instantly eliminated. 
    \item \textbf{Minimization of structural complexity}: Nodes are added to a minimal initial structure in order to prevent networks from growing in size without benefits.
\end{itemize}

\subsection{Crossover and Mutation}

\subsubsection{Mutation}
Additionally to weight mutation, NEAT relies on 2 types of structural mutations: adding connections, and adding nodes (figure \ref{fig:neat-mut}).

Adding a connection only requires creating a new gene between two neurons which were not previously linked. This new connection is assigned an incremented innovation number as a unique identifier across the whole population and all generations, which allows to track its existence in the NEAT run. 

Adding a node requires splitting a connection into 2 new ones, by creating a new neuron and adding two new connections as two new genes. 

\begin{figure}[H]
\centering
\captionsetup{justification=centering,margin=2cm}
\includegraphics[width=10cm]{images/neat_mut.png}
\caption{The two types of structural mutation in NEAT. Both types, adding a connection and adding a node, are illustrated with the genes above their phenotypes.  The top number in each genome is  the innovation number of that gene. New genes are assigned new increasingly higher innovation numbers. \cite{NEAT_2}}
 \label{fig:neat-mut}
\end{figure}

\subsubsection{Crossover}
NEAT crossover relies on the comparison of the two genomes based on the genes innovation numbers as unique identifiers (figure \ref{fig:neat-cros}). If both parents have a gene with the same innovation number (but possibly different weights), the algorithm selects one at random. If a gene only appears in one parent, it is selected if this parent has the better fitness.

In the case where multiple activation functions are possible (e.g. in a CPPN) the dictionaries linking each neuron to its function in each parent are merged, giving the priority to the fittest parent when a neuron appears in both. 

\begin{figure}[H]
\centering
\captionsetup{justification=centering,margin=2cm}
\includegraphics[width=10cm]{images/neat_cros.png}
\caption{NEAT crossover mechanism matching  up  genomes  for  different  network  topologies  using  innovation numbers.   Although  Parent 1  and  Parent 2  look  different,  their  innovation  numbers (shown at the top of each gene) tell us which genes match up with which. For matching genes, the one to keep is selected at random. \cite{NEAT_2}}
 \label{fig:neat-cros}
\end{figure}

\subsubsection{Mutation power}

The $mutation\_power$ parameter implements a stronger mutation amplitude on the weights of the last genes of the genome, as they are more recent. It considers older genes have had more time to be optimised, and that newer ones may require bigger steps to converge faster. It is not part of the original paper, but has been described on the author's website and in his code.

\subsection{Speciation}

\subsubsection{Stolen children}
The stolen children feature consists in removing a few offsprings from bad species and adding them to good ones, making the search more greedy. It is not part of the original paper, but has been described on the author's website and in his code.

\subsubsection{Elitism}
\label{subsec:elitism}
Elitism is the mechanism keeping the best individuals (\textit{elites}) from a generation to the next one, ensuring the best solution is not lost once encountered. \\
There is no elitism in the original implementation, although it has been added in the latest version of my library as an optional feature, for comparison purposes with CGP which uses it.

\section{Implementing NEAT}
I developed the NEAT algorithm based on the original papers \cite{NEAT_1} and \cite{NEAT_2}, taking into account the remarks and indications of the author on \addlink{https://www.cs.ucf.edu/~kstanley/neat.html#FAQ2}{his website}. \\
The \addlink{https://github.com/FernandoTorres/NEAT}{reorganized version} of the original C++ implementation of NEAT also provided insight into the algorithm, although some parts were approached differently, taking the papers as reference.

\subsection{NEAT Individual}

Based on Cambrian.jl, NeuroEvolution.jl uses an abstract type \code{NEATIndiv} from which the structure \code{NEATIndividual} inherits. This allows to build new NEAT-like individuals using the same methods with additional features, like HyperNEAT which relies on the NEAT algorithm for evolution, but integrates a different evaluation procedure. \code{NEATIndiv} inherits from \code{Cambrian.Individual} to use Cambrian's architecture.

A \code{NEATIndividual} object contains a dictionary mapping an innovation number to a gene, which represents a connection between two neurons, a list of neurons defined by their positions, a dictionary mapping each neuron position to the activation function of the corresponding neuron, a \code{Network} object containing the neural network built from the genome, and a list of the fitness of the individual as required by Cambrian.

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Julia, caption=NEAT Individual]
abstract type NEATIndiv <: Cambrian.Individual end

mutable struct NEATIndividual <: NEATIndiv
    genes::Dict
    fitness::Array{Float64}
    neuron\_pos::Array{Float64}
    network::Network
    activ\_functions::Dict
end
\end{lstlisting}
Source code at \url{https://github.com/TemplierPaul/NeuroEvolution.jl/blob/master/src/individual.jl}\\
\end{minipage}

\subsection{Neuron position and Execution order}
\label{subsec:NEAT_neur_pos}

On of the main issues encountered when implementing NEAT in previous personal projects is the order of computation of the neurons. Due to the way it grows, the NEAT network can build recurrent connections, including bidirectional relationships with one connection from neuron A to neuron B, and one connection from B to A. Then, propagating depth from the output neurons to make sure all previous neuron values have been computed leads to an infinite loop between A and B, each one needing to be computed before the other. 

To prevent that issue and reduce the time needed to build the network, this implementation of NEAT was inspired by the work of \cite{wilson2018positionalcgp} in which the position of a CGP node is described as a floating point value. The execution order is then available by design since they can be executed sequentially based on their positions. Recurrent connections are defined by destination neurons being before origins, which makes possible to allow or block them easily at mutation. 

This representation also removes the propagation time concept, making the NEAT network closer to standard neural networks. 

The position of a neuron is defined as follows:
\begin{itemize}
    \item All input neurons are placed with negative integer positions.  
    \item The bias neuron (constant equal to 1) is placed at $0$.  
    \item All output neurons are placed at positive integer positions, starting at $1$.
    \item All other neurons are placed with positions in $]0, 1[$.
\end{itemize}

\begin{figure}[H]
\centering
\captionsetup{justification=centering,margin=2cm}
\includegraphics[width=10cm]{images/NEAT_position.png}
\caption{Example of NEAT individual as neurons defined by position and feed-forward connections, with 2 input features, 1 output neuron and 2 hidden neurons.}
\includegraphics[width=10cm]{images/NEAT_network.png}
\caption{Neural Network generated from the above individual.}
 \label{NEAT_network}
\end{figure}

Neurons added through mutation of a connection are placed at a random position between the origin node of the connection at $p\_o$ and the destination node at $p\_d$, so that the order of execution is not modified. \\
The position of the new neuron is therefore randomly drawn in $]p\_o, p\_d[ \: \cap \: ]0, 1[$.

Because of this structure, and to avoid edge cases, connections between 2 output neurons are forbidden. Connections towards input and bias neurons are blocked too, since their values are fixed to the input vector and to 1 respectively.

When computing the network's output, its neurons are sorted by ascending position. Since the position determines the neuron type, choosing the update type is simple:

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Julia, caption=NEAT network processing]
for p in indiv.neuron\_pos
    if p < 0 # Input neurons
        indiv.network.neurons[p].output = 
                    last\_features[Int(-p)]
    elseif p == 0 # Bias neuron
        indiv.network.neurons[p].output = 1
    else # Hidden and output neurons
        compute!(indiv.network.neurons[p], 
                    indiv.network.neurons)
    end
end
\end{lstlisting}
Source code at \url{https://github.com/TemplierPaul/NeuroEvolution.jl/blob/master/src/process.jl}\\
\end{minipage}

\subsection{Run configuration}

The parameters to run NEAT are defined in a \addlink{https://github.com/TemplierPaul/NeuroEvolution.jl/blob/master/cfg/test.yaml}{YAML file}  that is read at runtime. It includes:

\begin{itemize}
    \item Cambrian settings: population size, number of generations
    \item Input and output size of the network
    \item Speciation parameters
    \item Mutation probabilities for each mutation type
    \item Available activation functions including sigmoid, trigonometric functions, absolute value, identity, or ReLU. 
\end{itemize}

One of the available activation functions is randomly selected when a new neuron is created, allowing the implementation of a Compositional Pattern-Producing Network (CPPN) by making multiple ones available. The default activation function for NEAT is the sigmoid. 

\subsection{Crossover and innovation number}
A usually delicate part of the implementation of NEAT lies in the management of the innovation number, which tracks genes across the population and guarantees the same gene is recognized on both sides during crossover.

In this implementation, and according to Cambrian's structure, a configuration dictionary is passed across the algorithm to each method requiring run parameters, built from the YAML configuration file. The innovation number is stored inside and updated after each mutation adding a gene. 

Using a dictionary in the individual to store each gene linked to its innovation number allows to compare genomes more efficiently at crossover than an array of genes. 

\subsection{Speciation threshold}
NEAT relies on speciation to update its population:
\begin{itemize}
    \item If individuals are closer than the speciation threshold, they get put in the same species
    \item The fitness of each individual is computed with explicit fitness sharing, hence divided by the size of the population of their species
    \item Offspring are distributed among species depending on the mean (or the max) fitness in the current members of the species
    \item Reproduction is computed in each species by selecting parents randomly in the top members of the species (or based on their individual fitness in a tournament selection) and applying either mutation or crossover.
    \item All offspring are assigned to the closest species
\end{itemize}

However as generations occur the distance between genomes grows and the algorithm tends to assign a different species to each individual, hence causing hundreds of species to appear. In order to keep a stable number of species, K.O.Stanley introduces on his website a changing threshold, increasing of decreasing by a fix amount in order to keep the number of active species stable. 

I introduced a slightly modified version of this algorithm as \code{update\_threshold!}, which modifies the threshold by an amount scaled on the ratio: $$current\_species\_number \over wanted\_species\_number$$

This solution yields a very stable number of species, although usually higher than the wanted number.

\subsection{Compositional Pattern-Producing Networks}

Compositional Pattern-Producing Networks \cite{CPPN}, or CPPNs, are neural networks built with multiple possible activation functions, including \code{sin}, \code{cos}, or \code{tanh}, producing patterns with symmetries. 

CPPNs can be evolved with CPPN-NEAT, a variant of NEAT where multiple activation functions are used, being determined when the node is created in a mutation. This architecture is used in HyperNEAT so that the final network can benefit from the symmetries. 

As a standard NEAT is a CPPN with a single activation function, I implemented the possibility to have CPPNs in the base NEAT code, and the activation functions can be selected in the \addlink{https://github.com/TemplierPaul/NeuroEvolution.jl/blob/master/cfg/test.yaml}{YAML config file}.

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Julia, caption=Available activation functions in my implementation]
# Activation functions
sigmoid: true
sin: false
cos: false
ReLU: false
tanh: false
abs: false
identity: false
gauss: false
\end{lstlisting}
Source code at \url{https://github.com/TemplierPaul/NeuroEvolution.jl/blob/master/cfg/test.yaml}\\
\end{minipage}

\subsection{NEAT in a standard Genetic Algorithm}
\label{sec:neat-ga}

The goal of using Cambrian as a framework in this implementation also was to allow for a high compatibility with other algorithm so that the coding blocks defined for NEAT could be reused in classical evolutionary algorithms, from a Genetic Algorithm (GA) to a Quality-Diversity method as envisioned in the Breezy competition (cf chapter \ref{chap:dota}). 

Hence, I finally implemented a simple wrapper to use the NEAT Individual and its mutation and crossover operators in a standard Genetic Algorithm, ignoring the specific species-based populate function for the standard Cambrian \code{ga\_populate} function.

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Julia, caption=Genetic Algorithm populate for NEAT]
function neat\_ga\_populate!(e::Evolution)
    mut::Function=i::NEATIndiv -> 
                    NeuroEvolution.mutate(i, e.cfg)
    cross::Function=(p1, p2) -> 
                    NeuroEvolution.crossover(p1, p2, e.cfg)
    Cambrian.ga\_populate!(e, mutation=mut, crossover=cross)
end
\end{lstlisting}
Source code at \url{https://github.com/TemplierPaul/NeuroEvolution.jl/blob/master/src/evolution.jl}\\
\end{minipage}

This compatibility with other Cambrian algorithms allows to easily use the NEAT individual in other settings such as Quality-Diversity methods or open-ended evolution, thus helping in new algorithms research.

\section{HyperNEAT}
HyperNEAT \cite{HyperNEAT} introduces indirect encoding by using a CPPN (Compositional Pattern-Producing Network \cite{CPPN}) to determine the weights of a neural network with a fix architecture. Since the CPPN can be evolved with the NEAT algorithm for HyperNEAT, most evolutionary methods in NeuroEvolution.jl can be reused. \\ 
However, instead of evaluating the evolved network (the CPPN) to solve the problem, the CPPN is used to determine the weights of a network with fixed architecture, referenced here as the \code{final network}. 

\subsection{HyperNEAT Individual}

HyperNEAT was added to NeuroEvolution.jl by creating a structure \code{HyperNEATIndiv}, inheriting from \code{NEATIndividual} like \code{NEATIndiv}. Thus, functions affecting the evolved network (e.g. mutation, crossover, populate) take as arguments any object inheriting from the abstract type \code{NEATIndividual}, and are blind to whether NEAT or HyperNEAT is used. Conversely, other functions related to the evaluation of an individual (e.g. evaluate, build, process) are algorithm-specific and are defined for each individual type.

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Julia, caption=HyperNEAT Individual]
mutable struct HyperNEATIndividual <: NEATIndiv
    genes::Dict
    fitness::Array{Float64}
    neuron\_pos::Array{Float64}
    network::Network
    activ\_functions::Dict
    hn\_net::GridNetwork
end
\end{lstlisting}
Source code at \url{https://github.com/TemplierPaul/NeuroEvolution.jl/blob/master/src/HyperNEAT.jl}\\
\end{minipage}

\code{HyperNEATIndividual} uses the same structure as \code{NEATIndividual}, adding a \code{hn\_network} field to store the generated final network described in subsection \ref{sub:final-hn-net}.

\subsection{Final network definition}
\label{sub:final-hn-net}
The final network is a feed-forward, fully-connected neural network with a fixed architecture. It is build from an array of \code{Layer} objects, each containing the output values of a neurons layer, and an array of \code{Connection} objects linking 2 layers and implementing weights. 

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Julia, caption=HyperNEAT final network]
mutable struct GridNetwork
    layers::Array{Layer}
    connections::Array{Connection}
end
\end{lstlisting}
Source code at \url{https://github.com/TemplierPaul/NeuroEvolution.jl/blob/master/src/HyperNEAT.jl}\\
\end{minipage}

The \code{build!} method creates the final network and sets its weighs with the CPPN, while the \code{process!} method computes the output of the final network. This approach allows to only build the network once, but increases complexity by adding a step to place between mutation and evaluation.

\subsection{Run configuration}

Using HyperNEAT requires a \addlink{https://github.com/TemplierPaul/NeuroEvolution.jl/blob/master/cfg/hyperneat.yaml}{specific YAML file} which adds information about hidden layers and structure of the generated network to the standard NEAT parameters. 

Although the default architecture for the final networks only links a layer of neurons to the previous one, toggling the \code{hn\_link\_all\_layers} parameter to \code{true} creates a connection between each layer and all the previous ones, for a more complex architecture. Using a CPPN for weights generation makes it possible to use such architecture without expanding the search space, contrary to direct encoding. 

\section{Future developments}

As NeuroEvolution.jl still seems to struggle on harder RL tasks in BERL compared to CGP, some additional features are planned for future development. This includes using the HyperNEAT CPPN to generate biases in the final network as well as connection weights, and adding stolen children to make the search more greedy or implementing mutation power, which have been described in the original code but not in the published work.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "isae-report-template"
%%% End: 